{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Blueoil on Amazon SageMaker\n",
    "## Docker build and push (to Amazon ECR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please add 2 lines in Dockerfile:\n",
    "# ENV USE_HOROVOD=\"True\"\n",
    "# ENV CUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7\"\n",
    "\n",
    "!bash ./docker_push_ecr.sh blueoil-sagemaker-dist blueoil/blueoil:v0.22.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data (upload a face images subset of OpenimagesV4 to Amazon S3)\n",
    "### Create sagemaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def upload_data(sess, path, key_prefix='data', compress=False):\n",
    "    if compress:\n",
    "        path = shutil.make_archive(path, 'gztar', '.', path)\n",
    "    s3_data = sess.upload_data(path=path, key_prefix=key_prefix)\n",
    "    return s3_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a face image subset of OpenimagesV4 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -O https://s3-ap-northeast-1.amazonaws.com/leapmind-public-storage/datasets/openimages_classification.tgz\n",
    "# !tar xf openimages_classification.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train_data = upload_data(sess, 'openimagesv4_10', compress=True)\n",
    "\n",
    "# ==== delete later ====\n",
    "# single dataset channel with compression \n",
    "dataset = 's3://hariby-iad/dataset/openimagesv4_10.tar.gz'\n",
    "# dataset = 'file:///home/ec2-user/SageMaker/dataset'\n",
    "# dataset = 'file:///fsx/dataset'\n",
    "\n",
    "# train/validation in separate channel\n",
    "# hariby test\n",
    "# train_data = 's3://hariby-iad/dataset/openimagesv4_10/train'\n",
    "# validation_data = 's3://hariby-iad/dataset/openimagesv4_10/validation'\n",
    "# # for local test\n",
    "# train_data = 'file:///home/ec2-user/SageMaker/openimagesv4_10/train/'\n",
    "# validation_data = 'file:///home/ec2-user/SageMaker/openimagesv4_10/validation/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing config (upload to Amazon S3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create config file\n",
    "https://docs.blueoil.org/tutorial/image_det.html\n",
    "\n",
    "create `openimages_objectdetection_sample.py` \n",
    "\n",
    "by \n",
    "```\n",
    "blueoil init -o openimages_objectdetection_sample.py\n",
    "```\n",
    "with\n",
    "```\n",
    "dataset:\n",
    "  format:                  OpenImagesV4\n",
    "  train dataset path:      /opt/ml/input/data/dataset/train/\n",
    "  validation dataset path: /opt/ml/input/data/dataset/validation/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "config_data = upload_data(sess, 'openimages_objectdetection_sample.py', key_prefix='config', compress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve data location\n",
    "\n",
    "# train_data  = 's3://' + sagemaker.Session().default_bucket() + '/data/openimages_face.tar.gz'\n",
    "# config_data += 's3://' + sagemaker.Session().default_bucket() + '/config/openimages_face_sample.yml'\n",
    "print(config_data)\n",
    "print(dataset)\n",
    "# print(train_data)\n",
    "# print(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On Amazon Sagemaker on-demand instance\n",
    "#### Create session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "algorithm_name = 'blueoil-sagemaker-dist'\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "ecr_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storage configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "# Specify EFS ile system id.\n",
    "file_system_id = 'fs-xxxxxxxxxxxxxxxxx' \n",
    "print(f\"FSx file-system-id: {file_system_id}\")\n",
    "\n",
    "# Specify directory path for input data on the file system. \n",
    "# You need to provide normalized and absolute path below.\n",
    "file_system_directory_path = '/xxxxxxxx/dataset'\n",
    "print(f'FSx file-system data input path: {file_system_directory_path}')\n",
    "\n",
    "# Specify the access mode of the mount of the directory associated with the file system. \n",
    "# Directory must be mounted  'ro'(read-only).\n",
    "file_system_access_mode = 'rw'\n",
    "\n",
    "# Specify your file system type\n",
    "file_system_type = 'FSxLustre'\n",
    "\n",
    "lustre = FileSystemInput(file_system_id=file_system_id,\n",
    "                                    file_system_type=file_system_type,\n",
    "                                    directory_path=file_system_directory_path,\n",
    "                                    file_system_access_mode=file_system_access_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# train_instance_type = 'local_gpu'\n",
    "train_instance_type = 'ml.p3.16xlarge'\n",
    "train_instance_count=1\n",
    "\n",
    "blueoil_experiment_id = 'objectdetection_opanimagesv4_10_sample'\n",
    "\n",
    "estimator = Estimator(\n",
    "    image_name=ecr_image, \n",
    "    role=sagemaker.get_execution_role(), \n",
    "    train_instance_count=train_instance_count, \n",
    "    train_instance_type=train_instance_type, \n",
    "    train_volume_size=256,  \n",
    "    hyperparameters={\n",
    "        'config': '/opt/ml/input/data/config/openimages_objectdetection_sample.py', \n",
    "        'experiment_id': blueoil_experiment_id\n",
    "    }, \n",
    "    subnets = ['subnet-xxxxxxxx'], \n",
    "    security_group_ids = ['sg-xxxxxxxx'], \n",
    "    train_max_run=5*24*60*60, \n",
    "    base_job_name=f'blueoil-sagemaker-dist-hvd-{train_instance_count}nodes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estimator.fit({'dataset': dataset, 'config': config_data})\n",
    "# estimator.fit({'train': train_data, 'validation': validation_data, 'config': config_data})\n",
    "\n",
    "# Lustre\n",
    "estimator.fit({'dataset': lustre, 'config': config_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert\n",
    "### On Amazon Sagemaker on-demand instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "convert_instance_type = 'ml.m5.xlarge'\n",
    "\n",
    "processor = ScriptProcessor(\n",
    "    image_uri=ecr_image,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    command=['python3'],\n",
    "    base_job_name=\"blueoil-convert\",\n",
    "    instance_count=1,\n",
    "    instance_type=convert_instance_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = estimator.model_data\n",
    "converted_model = os.path.join(os.path.dirname(trained_model), 'converted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(code='script/main.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(source=train_data, destination='/opt/ml/processing/input/data/dataset'),\n",
    "        ProcessingInput(source=estimator.model_data, destination='/opt/ml/processing/input/data/model'),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(source='/opt/ml/processing/output/converted', destination=converted_model),\n",
    "    ],\n",
    "    arguments=['convert', '--experiment_id', blueoil_experiment_id],\n",
    ")\n",
    "\n",
    "print(f\"Converted models are saved to {converted_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp $converted_model ./ --recursive\n",
    "!tar zxvf output.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright (c) 2020 LeapMind Inc. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
